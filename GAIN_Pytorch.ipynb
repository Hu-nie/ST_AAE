{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Packages\n",
    "import torch\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'data/V_228.csv'  # 'Letter.csv' for Letter dataset an 'Spam.csv' for Spam dataset\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.2\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "Data = np.loadtxt(dataset_file, delimiter=\",\",skiprows=1)\n",
    "\n",
    "# Parameters\n",
    "No = len(Data)\n",
    "Dim = len(Data[0,:])\n",
    "\n",
    "# Hidden state dimensions\n",
    "H_Dim1 = Dim\n",
    "H_Dim2 = Dim\n",
    "\n",
    "# Normalization (0 to 1)\n",
    "Min_Val = np.zeros(Dim)\n",
    "Max_Val = np.zeros(Dim)\n",
    "\n",
    "for i in range(Dim):\n",
    "    Min_Val[i] = np.min(Data[:,i])\n",
    "    #print(np.min(Data[:,i]))\n",
    "    Data[:,i] = Data[:,i] - np.min(Data[:,i])\n",
    "    Max_Val[i] = np.max(Data[:,i])\n",
    "    Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#%% Missing introducing\n",
    "p_miss_vec = p_miss * np.ones((Dim,1)) \n",
    "   \n",
    "Missing = np.zeros((No,Dim))\n",
    "\n",
    "for i in range(Dim):\n",
    "    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "    B = A > p_miss_vec[i]\n",
    "    Missing[:,i] = 1.*B\n",
    "\n",
    "    \n",
    "#%% Train Test Division    \n",
    "   \n",
    "idx = np.random.permutation(No)\n",
    "\n",
    "Train_No = int(No * train_rate)\n",
    "Test_No = No - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = Data[idx[:Train_No],:]\n",
    "testX = Data[idx[Train_No:],:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = Missing[idx[:Train_No],:]\n",
    "testM = Missing[idx[Train_No:],:]\n",
    "\n",
    "    \n",
    "# Hint Vector Generation\n",
    "def sample_M(m, n, p):\n",
    "    A = np.random.uniform(0., 1., size = [m, n])\n",
    "    B = A > p\n",
    "    C = 1.*B\n",
    "    return C\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return np.random.normal(size = size, scale = xavier_stddev)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(456, 228, bias=True)\n",
    "        self.fc2 = nn.Linear(228, 228, bias=True)\n",
    "        self.fc3 = nn.Linear(228, 228, bias=True)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        layers = [self.fc, self.fc2, self.fc3]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "    def forward(self,  New_X, M):\n",
    "        x = torch.cat(dim=1, tensors=[New_X, M])\n",
    "        x = x.float()\n",
    "        x = self.relu(self.fc(x))\n",
    "        outputs = self.relu(self.fc2(x))\n",
    "        outputs = self.relu(self.fc3(outputs))\n",
    "        return self.sigmoid(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(456, 228, bias=True)\n",
    "        self.fc2 = nn.Linear(228, 228, bias=True)\n",
    "        self.fc3 = nn.Linear(228, 228, bias=True)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        layers = [self.fc, self.fc2, self.fc3]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "    def forward(self,New_X, H):\n",
    "        x = torch.cat(dim=1, tensors=[New_X, H])\n",
    "        x = x.float()\n",
    "        x = self.relu(self.fc(x))\n",
    "        outputs = self.relu(self.fc2(x))\n",
    "        outputs = self.relu(self.fc3(outputs))\n",
    "        return self.sigmoid(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(0., 0.01, size = [m, n])        \n",
    "\n",
    "# Mini-batch generation\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(M, New_X, H):\n",
    "    # Combine with original data\n",
    "    G_sample = generator(New_X,M)\n",
    "    \n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    D_loss = -torch.mean(M * torch.log(D_prob + 1e-8) + (1-M) * torch.log(1. - D_prob + 1e-8))\n",
    "    return D_loss\n",
    "\n",
    "def generator_loss(X, M, New_X, H):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    G_loss1 = -torch.mean((1-M) * torch.log(D_prob + 1e-8))\n",
    "    MSE_train_loss = torch.mean((M * New_X - M * G_sample)**2) / torch.mean(M)\n",
    "\n",
    "    G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return G_loss, MSE_train_loss, MSE_test_loss\n",
    "    \n",
    "def test_loss(X, M, New_X):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return MSE_test_loss, G_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_D = torch.optim.Adam(params=generator.parameters(), lr = 1)\n",
    "optimizer_G = torch.optim.Adam(params=discriminator.parameters(), lr = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (fc): Linear(in_features=456, out_features=228, bias=True)\n",
       "  (fc2): Linear(in_features=228, out_features=228, bias=True)\n",
       "  (fc3): Linear(in_features=228, out_features=228, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "generator.apply(init_weights)\n",
    "    \n",
    "discriminator.apply(init_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d72c8b41bab4d5192cabee161bb65ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\tTrain_loss: 0.3242\tTest_loss: 0.3279\tG_loss: 1.074\tD_loss: 3.18\n",
      "Iter: 100\tTrain_loss: 0.3187\tTest_loss: 0.3192\tG_loss: 1.039\tD_loss: 3.222\n",
      "Iter: 200\tTrain_loss: 0.3162\tTest_loss: 0.3147\tG_loss: 1.023\tD_loss: 3.235\n",
      "Iter: 300\tTrain_loss: 0.3191\tTest_loss: 0.3209\tG_loss: 1.041\tD_loss: 3.185\n",
      "Iter: 400\tTrain_loss: 0.3165\tTest_loss: 0.3161\tG_loss: 1.025\tD_loss: 3.208\n",
      "Iter: 500\tTrain_loss: 0.3227\tTest_loss: 0.3199\tG_loss: 1.064\tD_loss: 3.229\n",
      "Iter: 600\tTrain_loss: 0.3201\tTest_loss: 0.3177\tG_loss: 1.048\tD_loss: 3.202\n",
      "Iter: 700\tTrain_loss: 0.3179\tTest_loss: 0.3205\tG_loss: 1.033\tD_loss: 3.148\n",
      "Iter: 800\tTrain_loss: 0.3166\tTest_loss: 0.3184\tG_loss: 1.024\tD_loss: 3.149\n",
      "Iter: 900\tTrain_loss: 0.321\tTest_loss: 0.3202\tG_loss: 1.053\tD_loss: 3.187\n",
      "Iter: 1000\tTrain_loss: 0.3225\tTest_loss: 0.3225\tG_loss: 1.063\tD_loss: 3.224\n",
      "Iter: 1100\tTrain_loss: 0.3182\tTest_loss: 0.3186\tG_loss: 1.034\tD_loss: 3.164\n",
      "Iter: 1200\tTrain_loss: 0.3211\tTest_loss: 0.3188\tG_loss: 1.054\tD_loss: 3.211\n",
      "Iter: 1300\tTrain_loss: 0.3129\tTest_loss: 0.3162\tG_loss: 1.001\tD_loss: 3.079\n",
      "Iter: 1400\tTrain_loss: 0.3182\tTest_loss: 0.3182\tG_loss: 1.035\tD_loss: 3.229\n",
      "Iter: 1500\tTrain_loss: 0.3175\tTest_loss: 0.3179\tG_loss: 1.03\tD_loss: 3.217\n",
      "Iter: 1600\tTrain_loss: 0.3157\tTest_loss: 0.3133\tG_loss: 1.02\tD_loss: 3.198\n",
      "Iter: 1700\tTrain_loss: 0.3244\tTest_loss: 0.3287\tG_loss: 1.075\tD_loss: 3.24\n",
      "Iter: 1800\tTrain_loss: 0.3188\tTest_loss: 0.3201\tG_loss: 1.039\tD_loss: 3.224\n",
      "Iter: 1900\tTrain_loss: 0.3153\tTest_loss: 0.3118\tG_loss: 1.016\tD_loss: 3.258\n",
      "Iter: 2000\tTrain_loss: 0.3223\tTest_loss: 0.3207\tG_loss: 1.062\tD_loss: 3.224\n",
      "Iter: 2100\tTrain_loss: 0.3152\tTest_loss: 0.3156\tG_loss: 1.014\tD_loss: 3.285\n",
      "Iter: 2200\tTrain_loss: 0.3259\tTest_loss: 0.3294\tG_loss: 1.084\tD_loss: 3.3\n",
      "Iter: 2300\tTrain_loss: 0.3221\tTest_loss: 0.3284\tG_loss: 1.06\tD_loss: 3.299\n",
      "Iter: 2400\tTrain_loss: 0.323\tTest_loss: 0.327\tG_loss: 1.066\tD_loss: 3.217\n",
      "Iter: 2500\tTrain_loss: 0.3182\tTest_loss: 0.3204\tG_loss: 1.035\tD_loss: 3.234\n",
      "Iter: 2600\tTrain_loss: 0.3086\tTest_loss: 0.3133\tG_loss: 0.9751\tD_loss: 3.207\n",
      "Iter: 2700\tTrain_loss: 0.3142\tTest_loss: 0.3121\tG_loss: 1.01\tD_loss: 3.27\n",
      "Iter: 2800\tTrain_loss: 0.3207\tTest_loss: 0.319\tG_loss: 1.051\tD_loss: 3.207\n",
      "Iter: 2900\tTrain_loss: 0.3185\tTest_loss: 0.32\tG_loss: 1.036\tD_loss: 3.216\n",
      "Iter: 3000\tTrain_loss: 0.3186\tTest_loss: 0.3169\tG_loss: 1.037\tD_loss: 3.222\n",
      "Iter: 3100\tTrain_loss: 0.3169\tTest_loss: 0.3119\tG_loss: 1.026\tD_loss: 3.232\n",
      "Iter: 3200\tTrain_loss: 0.3141\tTest_loss: 0.3154\tG_loss: 1.009\tD_loss: 3.213\n",
      "Iter: 3300\tTrain_loss: 0.3197\tTest_loss: 0.3216\tG_loss: 1.043\tD_loss: 3.203\n",
      "Iter: 3400\tTrain_loss: 0.3212\tTest_loss: 0.3245\tG_loss: 1.055\tD_loss: 3.211\n",
      "Iter: 3500\tTrain_loss: 0.3136\tTest_loss: 0.3113\tG_loss: 1.005\tD_loss: 3.294\n",
      "Iter: 3600\tTrain_loss: 0.3201\tTest_loss: 0.3143\tG_loss: 1.047\tD_loss: 3.167\n",
      "Iter: 3700\tTrain_loss: 0.3228\tTest_loss: 0.3198\tG_loss: 1.065\tD_loss: 3.22\n",
      "Iter: 3800\tTrain_loss: 0.3188\tTest_loss: 0.3227\tG_loss: 1.039\tD_loss: 3.159\n",
      "Iter: 3900\tTrain_loss: 0.3192\tTest_loss: 0.3194\tG_loss: 1.042\tD_loss: 3.135\n",
      "Iter: 4000\tTrain_loss: 0.3217\tTest_loss: 0.3259\tG_loss: 1.058\tD_loss: 3.258\n",
      "Iter: 4100\tTrain_loss: 0.3088\tTest_loss: 0.3087\tG_loss: 0.9767\tD_loss: 3.224\n",
      "Iter: 4200\tTrain_loss: 0.315\tTest_loss: 0.3196\tG_loss: 1.015\tD_loss: 3.199\n",
      "Iter: 4300\tTrain_loss: 0.3206\tTest_loss: 0.3207\tG_loss: 1.051\tD_loss: 3.241\n",
      "Iter: 4400\tTrain_loss: 0.3136\tTest_loss: 0.3176\tG_loss: 1.006\tD_loss: 3.179\n",
      "Iter: 4500\tTrain_loss: 0.3087\tTest_loss: 0.3102\tG_loss: 0.9775\tD_loss: 3.228\n",
      "Iter: 4600\tTrain_loss: 0.3209\tTest_loss: 0.3229\tG_loss: 1.052\tD_loss: 3.206\n",
      "Iter: 4700\tTrain_loss: 0.3278\tTest_loss: 0.3252\tG_loss: 1.096\tD_loss: 3.113\n",
      "Iter: 4800\tTrain_loss: 0.3203\tTest_loss: 0.3197\tG_loss: 1.048\tD_loss: 3.227\n",
      "Iter: 4900\tTrain_loss: 0.3218\tTest_loss: 0.319\tG_loss: 1.059\tD_loss: 3.206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%% Start Iterations\n",
    "for it in tqdm(range(5000)):    \n",
    "    \n",
    "    #%% Inputs\n",
    "    mb_idx = sample_idx(Train_No, mb_size)\n",
    "    X_mb = trainX[mb_idx,:]  \n",
    "    \n",
    "    Z_mb = sample_Z(mb_size, Dim) \n",
    "    M_mb = trainM[mb_idx,:]  \n",
    "    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "    H_mb =  M_mb * H_mb1 + 0.5*(1-H_mb1)\n",
    "    \n",
    "    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "    \n",
    "\n",
    "    X_mb = torch.tensor(X_mb)\n",
    "    M_mb = torch.tensor(M_mb)\n",
    "    H_mb = torch.tensor(H_mb)\n",
    "    New_X_mb = torch.tensor(New_X_mb)\n",
    "    \n",
    "    \n",
    "    optimizer_D.zero_grad()\n",
    "    D_loss_curr = discriminator_loss(M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    D_loss_curr.backward()\n",
    "    optimizer_D.step()\n",
    " \n",
    "    optimizer_G.zero_grad()\n",
    "    #print(M_mb.shape)\n",
    "    G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = generator_loss(X=X_mb, M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    G_loss_curr.backward()\n",
    "    optimizer_G.step()    \n",
    "        \n",
    "    #%% Intermediate Losses\n",
    "    if it % 100 == 0:\n",
    "        print('Iter: {}'.format(it),end='\\t')\n",
    "        print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr.item())),end='\\t')\n",
    "        print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr.item())),end='\\t')\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr),end='\\t')\n",
    "        print('D_loss: {:.4}'.format(D_loss_curr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_mb = sample_Z(Test_No, Dim) \n",
    "M_mb = testM\n",
    "X_mb = testX\n",
    "        \n",
    "New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "\n",
    "X_mb = torch.tensor(X_mb)\n",
    "M_mb = torch.tensor(M_mb)\n",
    "New_X_mb = torch.tensor(New_X_mb)\n",
    "\n",
    "    \n",
    "MSE_final, Sample = test_loss(X=X_mb, M=M_mb, New_X=New_X_mb)\n",
    "        \n",
    "print('Final Test RMSE: ' + str(np.sqrt(MSE_final.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = M_mb * X_mb + (1-M_mb) * Sample\n",
    "print(\"Imputed test data:\")\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.8f}\".format(x)})\n",
    "print(imputed_data.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization (0 to 1)\n",
    "renomal = imputed_data \n",
    "\n",
    "for i in range(Dim):\n",
    "    renomal[:,i] = renomal[:,i]* (Max_Val[i]+1e-6)\n",
    "    renomal[:,i] = renomal[:,i]+ Min_Val[i]\n",
    "    \n",
    "print(renomal.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(456, 228, bias=True)\n",
    "torch.nn.Parameter(torch.FloatTensor(xavier_init([228,456]))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2 = nn.Linear(456, 228, bias=True)\n",
    "nn.init.xavier_uniform_(fc2.weight,gain= (1. / np.sqrt(228 / 2.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
